{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from pickle import dump, load\n",
    "import string\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import add\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptionGenerator:\n",
    "    \n",
    "    IMAGE_WIDTH = 0\n",
    "    IMAGE_HEIGHT = 0\n",
    "    \n",
    "    IMAGE_CLASSIFIER = None\n",
    "    \n",
    "    def __init__(self, image_width, image_height):\n",
    "        self.IMAGE_WIDTH = image_width\n",
    "        self.IMAGE_HEIGHT = image_height\n",
    "        \n",
    "        #self.IMAGE_CLASSIFIER = ImageClassifier(image_width, image_height)\n",
    "        #self.IMAGE_CLASSIFIER.loadModel()\n",
    "        \n",
    "    def calculateDescriptionMaxLength(self, descriptions):\n",
    "        lines = self.convertToLines(descriptions)\n",
    "        return max(len(d.split()) for d in lines)\n",
    "        \n",
    "    def cleanDescriptions(self, descriptions):\n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        for key, description_list in descriptions.items():\n",
    "            for i in range(len(description_list)):\n",
    "                description = description_list[i]\n",
    "                description = description.split()\n",
    "                description = [word.lower() for word in description]\n",
    "                description = [w.translate(table) for w in description]\n",
    "                description = [word for word in description if len(word)>1]\n",
    "                description = [word for word in description if word.isalpha()]\n",
    "                description_list[i] = ' '.join(description)\n",
    "        \n",
    "    def convertPath(self, path):\n",
    "        validPath = \"\"\n",
    "        for char in path:\n",
    "            if char == '\\\\':\n",
    "                validPath += \"/\"\n",
    "            else:\n",
    "                validPath += char\n",
    "        return validPath\n",
    "    \n",
    "    def convertToLines(self, descriptions):\n",
    "        all_descriptions = list()\n",
    "        for key in descriptions.keys():\n",
    "            [all_descriptions.append(d) for d in descriptions[key]]\n",
    "        return all_descriptions\n",
    "    \n",
    "    def convertToVocabulary(self, descriptions):\n",
    "        all_descriptions = set()\n",
    "        for key in descriptions.keys():\n",
    "            [all_descriptions.update(d.split()) for d in descriptions[key]]\n",
    "        return all_descriptions\n",
    "    \n",
    "    def createSequences(self, tokenizer, max_length, description_list, photos, vocab_size):\n",
    "        X1, X2, y = list(), list(), list()\n",
    "        for description in description_list:\n",
    "            sequence = tokenizer.texts_to_sequences([description])[0]\n",
    "            for i in range(1, len(sequence)):\n",
    "                in_sequence, out_sequence = sequence[:i], sequence[i]\n",
    "                in_sequence = pad_sequences([in_sequence], maxlen=max_length)[0]\n",
    "                out_sequence = to_categorical([out_sequence], num_classes=vocab_size)[0]\n",
    "\n",
    "                X1.append(photos[key][0])\n",
    "                X2.append(in_sequence)\n",
    "                y.append(out_sequence)\n",
    "        return np.array(X1), np.array(X2), np.array(y)\n",
    "    \n",
    "    def createTokenizer(self, descriptions):\n",
    "        lines = self.convertToLines(descriptions)\n",
    "        tokenizer = Tokenizer()\n",
    "        tokenizer.fit_on_texts(lines)\n",
    "        return tokenizer\n",
    "    \n",
    "    def dataGenerator(self, descriptions, photos, tokenizer, max_length, vocab_size):\n",
    "        while 1:\n",
    "            for key, description_list in descriptions.items():\n",
    "                photo = photos[key][0]\n",
    "                in_img, in_seq, out_word = self.createSequences(tokenizer, max_length, descriptions, photo, vocab_size)\n",
    "                yield [in_img, in_seq], out_word\n",
    "    \n",
    "    def defineCaptioningModel(self, vocab_size, max_length, summarize=False):\n",
    "        # Feature extractor model\n",
    "        inputs1 = Input(shape=(4096,))\n",
    "        fe1 = Dropout(0.5)(inputs1)\n",
    "        fe2 = Dense(256, activation=\"relu\")(fe1)\n",
    "        \n",
    "        # Sequence model\n",
    "        inputs2 = Input(shape=(max_length,))\n",
    "        se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "        se2 = Dropout(0.5)(se1)\n",
    "        se3 = LSTM(256)(se2)\n",
    "        \n",
    "        # Decoder model\n",
    "        decoder1 = add([fe2, se3])\n",
    "        decoder2 = Dense(256, activation=\"relu\")(decoder1)\n",
    "        outputs = Dense(vocab_size, activation=\"softmax\")(decoder2)\n",
    "        \n",
    "        model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "        model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\")\n",
    "        \n",
    "        if summarize:\n",
    "            print(model.summary())\n",
    "            \n",
    "        plot_model(model, to_file=\"model.png\", show_shapes=True)\n",
    "        \n",
    "        return model\n",
    "        \n",
    "    def extractFeatures(self, directory, output_file=\"features.pkl\", verbose=1):\n",
    "        model =  model = VGG16()\n",
    "        model = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
    "        if(verbose >= 2):\n",
    "            print(model.summary())\n",
    "    \n",
    "        features = dict()\n",
    "        for name in os.listdir(directory):\n",
    "            filename = directory + '/' + name\n",
    "            image = load_img(filename, target_size=(self.IMAGE_WIDTH, self.IMAGE_HEIGHT))\n",
    "            image = img_to_array(image)\n",
    "            image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "            image = preprocess_input(image)\n",
    "            feature = model.predict(image, verbose=0)\n",
    "            image_id = name.split('.')[0]\n",
    "            features[image_id] = feature\n",
    "            if(verbose >= 3):\n",
    "                print('>%s' % name)\n",
    "                \n",
    "        if verbose >= 1:\n",
    "            print(\"Extracted Features: %d\" % len(features))\n",
    "            \n",
    "        dump(features, open(output_file, 'wb'))\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def fitCaptioningModel(self):\n",
    "        filepath = 'model-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5'\n",
    "        checkpoint = ModelCheckpoint(filepath, monitor=\"val_loss\", save_best_only=True, mode='min')\n",
    "    \n",
    "    def generateCaption(self, image_path, prediction_model_path=\"ImageClassifier/ImageClassifier.h5\"):\n",
    "        image_path = self.convertPath(image_path)\n",
    "        image = self.readImageFile(image_path)\n",
    "        self.IMAGE_CLASSIFIER.predict(image)\n",
    "        \n",
    "    def getTextFileContent(self, filename):\n",
    "        file = open(filename, 'r')\n",
    "        text = file.read()\n",
    "        file.close()\n",
    "        return text\n",
    "        \n",
    "    def loadCleanDescriptions(self, dataset, filename=\"Dataset/Descriptions.txt\"):\n",
    "        doc = self.getTextFileContent(filename)\n",
    "        descriptions = dict()\n",
    "        for line in doc.split('\\n'):\n",
    "            tokens = line.split()\n",
    "            image_id, image_description = tokens[0], tokens[1:]\n",
    "            if image_id in dataset:\n",
    "                if image_id not in descriptions:\n",
    "                    descriptions[image_id] = list()\n",
    "                description = 'startseq' + ' '.join(image_description) + 'endseq'\n",
    "                descriptions[image_id].append(description)\n",
    "        return descriptions\n",
    "\n",
    "    def loadDescriptions(self, filename):\n",
    "        text = self.getTextFileContent(filename)\n",
    "\n",
    "        mapping = dict()\n",
    "        for line in text.split('\\n'):\n",
    "            tokens = line.split()\n",
    "            if len(line) < 2:\n",
    "                continue\n",
    "            image_id, image_description = tokens[0], tokens[1:]\n",
    "            image_id = image_id.split('.')[0]\n",
    "            image_description = ' '.join(image_description)\n",
    "            if image_id not in mapping:\n",
    "                mapping[image_id] = list()\n",
    "            mapping[image_id].append(image_description)\n",
    "        return mapping\n",
    "    \n",
    "    def loadPhotoFeatures(self, filename, dataset):\n",
    "        all_features = load(open(filename, 'rb'))\n",
    "        features = {k: all_features[k] for k in dataset}\n",
    "        return features\n",
    "    \n",
    "    def loadSet(self, filename):\n",
    "        doc = self.getTextFileContent(filename)\n",
    "        dataset = list()\n",
    "        for line in doc.split('\\n'):\n",
    "            if len(line) < 1:\n",
    "                continue\n",
    "            identifier = line.split('.')[0]\n",
    "            dataset.append(identifier)\n",
    "        return set(dataset)\n",
    "    \n",
    "    def loadTrainingDataset(self):\n",
    "        # Train dataset\n",
    "        filename = \"Dataset/Text/Flickr_8k.trainImages.txt\"\n",
    "        train = self.loadSet(filename)\n",
    "        print(\"Dataset: %d\" % len(train))        \n",
    "        train_descriptions = self.loadCleanDescriptions(train)\n",
    "        print(\"Descriptions: train=%d\" %len(train_descriptions))\n",
    "        train_features = self.loadPhotoFeatures(\"Dataset/features.pkl\", train)\n",
    "        print(\"Photos: train=%d\" % len(train_features))        \n",
    "        tokenizer = self.createTokenizer(train_descriptions)\n",
    "        vocab_size = len(tokenizer.word_index) + 1\n",
    "        print(\"Vocabulary size: %d\" % vocab_size)        \n",
    "        max_length = self.calculateDescriptionMaxLength(train_descriptions)\n",
    "        print(\"Description length: %d\" % max_length)\n",
    "        X1_train, X2_train, y_train = self.createSequences(tokenizer, max_length, train_descriptions, train_features, vocab_size)\n",
    "        \n",
    "        # Test dataset\n",
    "        filename = \"Dataset/Text/Flickr_8k.devImages.txt\"\n",
    "        test = self.loadSet(filename)\n",
    "        print(\"Dataset: %d\" % len(test))\n",
    "        test_descriptions = self.loadCleanDescriptions(test)\n",
    "        print(\"Descriptions: test=%d\" % len(test_descriptions))\n",
    "        test_features = self.loadPhotoFeatures(\"Dataset/features.pkl\", test)\n",
    "        print(\"Photos: test=%d\" % len(test_features))\n",
    "        X1_test, X2_test, y_test = self.createSequences(tokenizer, max_length, test_descriptions, test_features, vocab_size)\n",
    "        \n",
    "        # Fit model\n",
    "        model = self.defineCaptioningModel(vocab_size, max_length, summarize=True)\n",
    "        #filepath = \"CaptionGenerator/fit/model-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5\"\n",
    "        #checkpoint = ModelCheckpoint(filepath, monitor=\"val_loss\", verbose=1, save_best_only=True, mode=\"min\")\n",
    "        epochs = 20\n",
    "        steps = len(train_descriptions)\n",
    "        for i in range(epochs):\n",
    "            generator = self.dataGenerator(train_descriptions, train_features, tokenizer, max_length, vocab_size)\n",
    "            model.fit(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n",
    "            model.save(\"CaptionGenerator/fit/model_\" + str(i) + \".h5\")\n",
    "    \n",
    "    def prepareImageCaptioningTrainDataset(self):\n",
    "        descriptions = self.loadDescriptions(\"Dataset/Text/Flickr8k.token.txt\")\n",
    "        self.cleanDescriptions(descriptions)\n",
    "        vocabulary = self.convertToVocabulary(descriptions)\n",
    "        self.saveDescriptions(descriptions)\n",
    "    \n",
    "    def readImageFile(self, image_path):\n",
    "        if not os.path.isfile(image_path):\n",
    "            print(\"Unknow path\")\n",
    "            return None\n",
    "        else:\n",
    "            image = cv2.imread(image_path)\n",
    "            image = cv2.resize(image, (self.IMAGE_WIDTH, self.IMAGE_HEIGHT))\n",
    "            image = np.expand_dims(image, axis=0)\n",
    "            return image\n",
    "        \n",
    "    def saveDescriptions(self, descriptions, filename=\"Dataset/Descriptions.txt\"):\n",
    "        lines = list()\n",
    "        for key, description_list in descriptions.items():\n",
    "            for description in description_list:\n",
    "                lines.append(key + ' ' + description)\n",
    "        data = '\\n'.join(lines)\n",
    "        file = open(filename, 'w')\n",
    "        file.write(data)\n",
    "        file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 6000\n",
      "Descriptions: train=6000\n",
      "Photos: train=6000\n",
      "Vocabulary size: 10400\n",
      "Description length: 32\n",
      "Dataset: 1000\n",
      "Descriptions: test=1000\n",
      "Photos: test=1000\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            [(None, 32)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 4096)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 32, 256)      2662400     input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 4096)         0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 32, 256)      0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 256)          1048832     dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 256)          525312      dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 256)          0           dense_3[0][0]                    \n",
      "                                                                 lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 256)          65792       add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 10400)        2672800     dense_4[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 6,975,136\n",
      "Trainable params: 6,975,136\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 4096) for input KerasTensor(type_spec=TensorSpec(shape=(None, 4096), dtype=tf.float32, name='input_3'), name='input_3', description=\"created by layer 'input_3'\"), but it was called on an input with incompatible shape (None, 1).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 32) for input KerasTensor(type_spec=TensorSpec(shape=(None, 32), dtype=tf.float32, name='input_4'), name='input_4', description=\"created by layer 'input_4'\"), but it was called on an input with incompatible shape (None, 1).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    C:\\Users\\sebas\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:840 train_function  *\n        return step_function(self, iterator)\n    C:\\Users\\sebas\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:830 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\sebas\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1262 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\sebas\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2734 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\sebas\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3423 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\sebas\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:823 run_step  **\n        outputs = model.train_step(data)\n    C:\\Users\\sebas\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:789 train_step\n        y_pred = self(x, training=True)\n    C:\\Users\\sebas\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:1025 __call__\n        outputs = call_fn(inputs, *args, **kwargs)\n    C:\\Users\\sebas\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py:427 call\n        return self._run_internal_graph(\n    C:\\Users\\sebas\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py:563 _run_internal_graph\n        outputs = node.layer(*args, **kwargs)\n    C:\\Users\\sebas\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:1011 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    C:\\Users\\sebas\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py:255 assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer dense_3 is incompatible with the layer: expected axis -1 of input shape to have value 4096 but received input with shape (None, 1)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-cd8015928d65>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mcaptionGenerator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCaptionGenerator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mIMAGE_WIDTH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIMAGE_HEIGHT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mcaptionGenerator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloadTrainingDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-11-e70a7c0470ca>\u001b[0m in \u001b[0;36mloadTrainingDataset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             \u001b[0mgenerator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataGenerator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_descriptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 225\u001b[1;33m             \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    226\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"CaptionGenerator/fit/model_\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\".h5\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1133\u001b[0m                 _r=1):\n\u001b[0;32m   1134\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1135\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1136\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    795\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    796\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 797\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    798\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    799\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    839\u001b[0m       \u001b[1;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    840\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 841\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    842\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    843\u001b[0m       \u001b[1;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    692\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    693\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m--> 694\u001b[1;33m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m    695\u001b[0m             *args, **kwds))\n\u001b[0;32m    696\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2975\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2976\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2977\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2978\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2979\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3369\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3370\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3371\u001b[1;33m           \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3372\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3373\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3204\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3205\u001b[0m     graph_function = ConcreteFunction(\n\u001b[1;32m-> 3206\u001b[1;33m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[0;32m   3207\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3208\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    996\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    997\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 998\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    999\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1000\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    601\u001b[0m             \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 603\u001b[1;33m           \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    604\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    605\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    983\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 985\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    986\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    987\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    C:\\Users\\sebas\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:840 train_function  *\n        return step_function(self, iterator)\n    C:\\Users\\sebas\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:830 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\sebas\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1262 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\sebas\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2734 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\sebas\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3423 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\sebas\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:823 run_step  **\n        outputs = model.train_step(data)\n    C:\\Users\\sebas\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:789 train_step\n        y_pred = self(x, training=True)\n    C:\\Users\\sebas\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:1025 __call__\n        outputs = call_fn(inputs, *args, **kwargs)\n    C:\\Users\\sebas\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py:427 call\n        return self._run_internal_graph(\n    C:\\Users\\sebas\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py:563 _run_internal_graph\n        outputs = node.layer(*args, **kwargs)\n    C:\\Users\\sebas\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:1011 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    C:\\Users\\sebas\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py:255 assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer dense_3 is incompatible with the layer: expected axis -1 of input shape to have value 4096 but received input with shape (None, 1)\n"
     ]
    }
   ],
   "source": [
    "##### dataset_directory = \"C:/Users/sebas/OneDrive/Images/Datasets/Flickr8k/Dataset\"\n",
    "#directory = os.path.join(dataset_directory, \"Flicker8k_Dataset\")\n",
    "#features = extract_features(directory)\n",
    "#print('Extracted Features: %d' % len(features))\n",
    "#dump(features, open('features.pkl', 'wb'))\n",
    "\n",
    "captionGenerator = CaptionGenerator(IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "captionGenerator.loadTrainingDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 6000\n",
      "Descriptions: train=6000\n",
      "Photos: train=6000\n",
      "Vocabulary Size: 7579\n",
      "Description Length: 34\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            [(None, 34)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 4096)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 34, 256)      1940224     input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 4096)         0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 34, 256)      0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 256)          1048832     dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 256)          525312      dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 256)          0           dense_3[0][0]                    \n",
      "                                                                 lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 256)          65792       add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 7579)         1947803     dense_4[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 5,527,963\n",
      "Trainable params: 5,527,963\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n",
      "6000/6000 [==============================] - 4941s 823ms/step - loss: 5.1224\n",
      "6000/6000 [==============================] - 5123s 854ms/step - loss: 3.9018\n",
      "6000/6000 [==============================] - 5119s 853ms/step - loss: 3.6490\n",
      "6000/6000 [==============================] - 5822s 970ms/step - loss: 3.4969\n",
      "6000/6000 [==============================] - 5720s 953ms/step - loss: 3.3938\n",
      "5650/6000 [===========================>..] - ETA: 5:38 - loss: 3.3217"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-fc46655a43c6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    168\u001b[0m         \u001b[0mgenerator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_descriptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m         \u001b[1;31m# fit for one epoch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    171\u001b[0m         \u001b[1;31m# save model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Dataser/fit/model_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1133\u001b[0m                 _r=1):\n\u001b[0;32m   1134\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1135\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1136\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    795\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    796\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 797\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    798\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    799\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    823\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    824\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 825\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    826\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2948\u001b[0m       (graph_function,\n\u001b[0;32m   2949\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2950\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2951\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2952\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1928\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1929\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1930\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1931\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1932\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    554\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    555\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 556\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    557\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    558\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from pickle import load\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import add\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    " \n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text\n",
    " \n",
    "# load a pre-defined list of photo identifiers\n",
    "def load_set(filename):\n",
    "\tdoc = load_doc(filename)\n",
    "\tdataset = list()\n",
    "\t# process line by line\n",
    "\tfor line in doc.split('\\n'):\n",
    "\t\t# skip empty lines\n",
    "\t\tif len(line) < 1:\n",
    "\t\t\tcontinue\n",
    "\t\t# get the image identifier\n",
    "\t\tidentifier = line.split('.')[0]\n",
    "\t\tdataset.append(identifier)\n",
    "\treturn set(dataset)\n",
    " \n",
    "# load clean descriptions into memory\n",
    "def load_clean_descriptions(filename, dataset):\n",
    "\t# load document\n",
    "\tdoc = load_doc(filename)\n",
    "\tdescriptions = dict()\n",
    "\tfor line in doc.split('\\n'):\n",
    "\t\t# split line by white space\n",
    "\t\ttokens = line.split()\n",
    "\t\t# split id from description\n",
    "\t\timage_id, image_desc = tokens[0], tokens[1:]\n",
    "\t\t# skip images not in the set\n",
    "\t\tif image_id in dataset:\n",
    "\t\t\t# create list\n",
    "\t\t\tif image_id not in descriptions:\n",
    "\t\t\t\tdescriptions[image_id] = list()\n",
    "\t\t\t# wrap description in tokens\n",
    "\t\t\tdesc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
    "\t\t\t# store\n",
    "\t\t\tdescriptions[image_id].append(desc)\n",
    "\treturn descriptions\n",
    " \n",
    "# load photo features\n",
    "def load_photo_features(filename, dataset):\n",
    "\t# load all features\n",
    "\tall_features = load(open(filename, 'rb'))\n",
    "\t# filter features\n",
    "\tfeatures = {k: all_features[k] for k in dataset}\n",
    "\treturn features\n",
    " \n",
    "# covert a dictionary of clean descriptions to a list of descriptions\n",
    "def to_lines(descriptions):\n",
    "\tall_desc = list()\n",
    "\tfor key in descriptions.keys():\n",
    "\t\t[all_desc.append(d) for d in descriptions[key]]\n",
    "\treturn all_desc\n",
    " \n",
    "# fit a tokenizer given caption descriptions\n",
    "def create_tokenizer(descriptions):\n",
    "\tlines = to_lines(descriptions)\n",
    "\ttokenizer = Tokenizer()\n",
    "\ttokenizer.fit_on_texts(lines)\n",
    "\treturn tokenizer\n",
    " \n",
    "# calculate the length of the description with the most words\n",
    "def max_length(descriptions):\n",
    "\tlines = to_lines(descriptions)\n",
    "\treturn max(len(d.split()) for d in lines)\n",
    " \n",
    "# create sequences of images, input sequences and output words for an image\n",
    "def create_sequences(tokenizer, max_length, desc_list, photo, vocab_size):\n",
    "\tX1, X2, y = list(), list(), list()\n",
    "\t# walk through each description for the image\n",
    "\tfor desc in desc_list:\n",
    "\t\t# encode the sequence\n",
    "\t\tseq = tokenizer.texts_to_sequences([desc])[0]\n",
    "\t\t# split one sequence into multiple X,y pairs\n",
    "\t\tfor i in range(1, len(seq)):\n",
    "\t\t\t# split into input and output pair\n",
    "\t\t\tin_seq, out_seq = seq[:i], seq[i]\n",
    "\t\t\t# pad input sequence\n",
    "\t\t\tin_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "\t\t\t# encode output sequence\n",
    "\t\t\tout_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "\t\t\t# store\n",
    "\t\t\tX1.append(photo)\n",
    "\t\t\tX2.append(in_seq)\n",
    "\t\t\ty.append(out_seq)\n",
    "\treturn array(X1), array(X2), array(y)\n",
    " \n",
    "# define the captioning model\n",
    "def define_model(vocab_size, max_length):\n",
    "\t# feature extractor model\n",
    "\tinputs1 = Input(shape=(4096,))\n",
    "\tfe1 = Dropout(0.5)(inputs1)\n",
    "\tfe2 = Dense(256, activation='relu')(fe1)\n",
    "\t# sequence model\n",
    "\tinputs2 = Input(shape=(max_length,))\n",
    "\tse1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "\tse2 = Dropout(0.5)(se1)\n",
    "\tse3 = LSTM(256)(se2)\n",
    "\t# decoder model\n",
    "\tdecoder1 = add([fe2, se3])\n",
    "\tdecoder2 = Dense(256, activation='relu')(decoder1)\n",
    "\toutputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "\t# tie it together [image, seq] [word]\n",
    "\tmodel = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "\t# compile model\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\t# summarize model\n",
    "\tmodel.summary()\n",
    "\tplot_model(model, to_file='model.png', show_shapes=True)\n",
    "\treturn model\n",
    " \n",
    "# data generator, intended to be used in a call to model.fit_generator()\n",
    "def data_generator(descriptions, photos, tokenizer, max_length, vocab_size):\n",
    "\t# loop for ever over images\n",
    "\twhile 1:\n",
    "\t\tfor key, desc_list in descriptions.items():\n",
    "\t\t\t# retrieve the photo feature\n",
    "\t\t\tphoto = photos[key][0]\n",
    "\t\t\tin_img, in_seq, out_word = create_sequences(tokenizer, max_length, desc_list, photo, vocab_size)\n",
    "\t\t\tyield [in_img, in_seq], out_word\n",
    " \n",
    "#load training dataset (6K)\n",
    "filename = 'Dataset/Text/Flickr_8k.trainImages.txt'\n",
    "train = load_set(filename)\n",
    "print('Dataset: %d' % len(train))\n",
    "# descriptions\n",
    "train_descriptions = load_clean_descriptions('Dataset/descriptions.txt', train)\n",
    "print('Descriptions: train=%d' % len(train_descriptions))\n",
    "# photo features\n",
    "train_features = load_photo_features('Dataset/features.pkl', train)\n",
    "print('Photos: train=%d' % len(train_features))\n",
    "# prepare tokenizer\n",
    "tokenizer = create_tokenizer(train_descriptions)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary Size: %d' % vocab_size)\n",
    "# determine the maximum sequence length\n",
    "max_length = max_length(train_descriptions)\n",
    "print('Description Length: %d' % max_length)\n",
    " \n",
    "# define the model\n",
    "model = define_model(vocab_size, max_length)\n",
    "# train the model, run epochs manually and save after each epoch\n",
    "epochs = 20\n",
    "steps = len(train_descriptions)\n",
    "for i in range(epochs):\n",
    "\t# create the data generator\n",
    "\tgenerator = data_generator(train_descriptions, train_features, tokenizer, max_length, vocab_size)\n",
    "\t# fit for one epoch\n",
    "\tmodel.fit(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n",
    "\t# save model\n",
    "\tmodel.save('Dataset/fit/model_' + str(i) + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageClassifier:\n",
    "    \n",
    "    DATASETS_PATH = \"D:/Cours/Cesi/A5/UE/Option - Data Science/Projet/Livrable 2/Datasets/extracted\"\n",
    "    CLASS_NAMES = ['Painting', 'Photo', 'Schematics', 'Sketch', 'Text']\n",
    "    \n",
    "    VALIDATION_SPLIT = 0.3\n",
    "    \n",
    "    IMAGE_WIDTH = 0\n",
    "    IMAGE_HEIGHT = 0\n",
    "    \n",
    "    MODEL = None\n",
    "    EPOCHS = 0\n",
    "    HISTORY = None\n",
    "    \n",
    "    def __init__(self, image_width, image_height):\n",
    "        self.IMAGE_WIDTH = image_width\n",
    "        self.IMAGE_HEIGHT = image_height\n",
    "        \n",
    "    def buildModel(self, dropout_rate=0, kernel_regularizer_l1=0.00, kernel_regularizer_l2=0.0):\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.experimental.preprocessing.Rescaling(1./255, input_shape=(self.IMAGE_HEIGHT, self.IMAGE_WIDTH, 3)),\n",
    "            tf.keras.layers.experimental.preprocessing.RandomFlip(\"horizontal\", input_shape=(self.IMAGE_HEIGHT, self.IMAGE_WIDTH, 3)),\n",
    "            tf.keras.layers.experimental.preprocessing.RandomRotation(10),\n",
    "            tf.keras.layers.experimental.preprocessing.RandomZoom((0.2, 0.5)),\n",
    "            tf.keras.layers.Conv2D(16, 3, padding=\"same\", activation=\"relu\", \n",
    "                                   kernel_regularizer=tf.keras.regularizers.l1_l2(l1=kernel_regularizer_l1, l2=kernel_regularizer_l2)),\n",
    "            tf.keras.layers.MaxPooling2D((2,2), padding='same'),\n",
    "            tf.keras.layers.Conv2D(32, 3, padding=\"same\", activation=\"relu\", \n",
    "                                   kernel_regularizer=tf.keras.regularizers.l1_l2(l1=kernel_regularizer_l1, l2=kernel_regularizer_l2)),\n",
    "            tf.keras.layers.MaxPooling2D((2,2), padding='same'),\n",
    "            tf.keras.layers.Conv2D(64, 3, padding=\"same\", activation=\"relu\", \n",
    "                                   kernel_regularizer=tf.keras.regularizers.l1_l2(l1=kernel_regularizer_l1, l2=kernel_regularizer_l2)),\n",
    "            tf.keras.layers.MaxPooling2D((2,2), padding='same'),\n",
    "            tf.keras.layers.Dropout(dropout_rate),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(128, activation=\"relu\", \n",
    "                                  kernel_regularizer=tf.keras.regularizers.l1_l2(l1=kernel_regularizer_l1, l2=kernel_regularizer_l2)),\n",
    "            tf.keras.layers.Dense(len(self.CLASS_NAMES))\n",
    "        ])\n",
    "        \n",
    "        model.compile(optimizer=\"adam\",\n",
    "                           loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                           metrics=[\"accuracy\"])\n",
    "        return model\n",
    "        \n",
    "    def fit(self, epochs, save_path=None, show_training_results=False, dropout_rate=0, kernel_regularizer_l1=0.01, kernel_regularizer_l2=0.01):\n",
    "        train_dataset, test_dataset = self.generateDatasets()\n",
    "        self.MODEL = self.buildModel(dropout_rate, kernel_regularizer_l1, kernel_regularizer_l2)\n",
    "        self.EPOCHS = epochs\n",
    "        \n",
    "        callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=2)\n",
    "        self.HISTORY = self.MODEL.fit(train_dataset, validation_data=test_dataset, epochs=epochs, callbacks=[callback])\n",
    "        \n",
    "        if save_path != None:\n",
    "            self.MODEL.save(save_path)\n",
    "            \n",
    "        if show_training_results:\n",
    "            self.showTrainingResults()\n",
    "    \n",
    "    def generateDatasets(self):\n",
    "        datasets = []\n",
    "        \n",
    "        for subset_label in ['training', 'validation']:\n",
    "            datasets.append(tf.keras.preprocessing.image_dataset_from_directory(\n",
    "            self.DATASETS_PATH,\n",
    "            labels=\"inferred\",\n",
    "            label_mode=\"int\",\n",
    "            validation_split=self.VALIDATION_SPLIT,\n",
    "            subset=subset_label,\n",
    "            seed=42,\n",
    "            color_mode=\"rgb\",\n",
    "            image_size=(self.IMAGE_WIDTH, self.IMAGE_HEIGHT)))\n",
    "        return datasets[0], datasets[1]\n",
    "    \n",
    "    def loadModel(self, model_path=\"ImageClassifier/ImageClassifier.h5\", add_softmax_layer=True):\n",
    "        model = tf.keras.models.load_model(model_path)\n",
    "        if add_softmax_layer:\n",
    "            model.add(tf.keras.layers.Softmax())\n",
    "        self.MODEL = model\n",
    "        \n",
    "    def modelSummary(self):\n",
    "        self.MODEL.summary()\n",
    "        \n",
    "    def predict(self, image, model_path=\"ImageClassifier/ImageClassifier.h5\"):\n",
    "        prediction = self.MODEL.predict(tf.convert_to_tensor(image))\n",
    "        print(self.CLASS_NAMES[np.argmax(prediction)])\n",
    "        \n",
    "    def showTrainingResults(self):\n",
    "        epochs_range = range(self.EPOCHS)\n",
    "    \n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(epochs_range, self.HISTORY.history['accuracy'], label=\"Training accuracy\")\n",
    "        plt.plot(epochs_range, self.HISTORY.history['val_accuracy'], label=\"Validation accuracy\")\n",
    "        plt.legend()\n",
    "        plt.title(\"Training and validation accuracy\")\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(epochs_range, self.HISTORY.history['loss'], label=\"Training loss\")\n",
    "        plt.plot(epochs_range, self.HISTORY.history['val_loss'], label=\"Validation loss\")\n",
    "        plt.legend()\n",
    "        plt.title(\"Training and validation loss\")\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_WIDTH = 224\n",
    "IMAGE_HEIGHT = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 49024 files belonging to 5 classes.\n",
      "Using 34317 files for training.\n",
      "Found 49024 files belonging to 5 classes.\n",
      "Using 14707 files for validation.\n",
      "Epoch 1/2\n",
      "1073/1073 [==============================] - 2356s 2s/step - loss: 0.8227 - accuracy: 0.6602 - val_loss: 1.4031 - val_accuracy: 0.5856\n",
      "Epoch 2/2\n",
      "1073/1073 [==============================] - 2529s 2s/step - loss: 0.4531 - accuracy: 0.8157 - val_loss: 2.4598 - val_accuracy: 0.5810\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_classifier = ImageClassifier(IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "image_classifier.fit(epochs=2, save_path=\"ImageClassifier/ImageClassifier1.h5\", show_training_results=True, \n",
    "                     dropout_rate=0, kernel_regularizer_l1=0.0, kernel_regularizer_l2=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_classifier = ImageClassifier(IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "image_classifier.fit(epochs=30, save_path=\"ImageClassifier/ImageClassifier2.h5\", show_training_results=True, \n",
    "                     dropout_rate=0.2, kernel_regularizer_l1=0.0, kernel_regularizer_l2=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_classifier = ImageClassifier(IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "image_classifier.fit(epochs=30, save_path=\"ImageClassifier/ImageClassifier3.h5\", show_training_results=True, \n",
    "                     dropout_rate=0.5, kernel_regularizer_l1=0.0, kernel_regularizer_l2=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_classifier = ImageClassifier(IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "image_classifier.fit(epochs=30, save_path=\"ImageClassifier/ImageClassifier4.h5\", show_training_results=True, \n",
    "                     dropout_rate=0.8, kernel_regularizer_l1=0.0, kernel_regularizer_l2=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_classifier = ImageClassifier(IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "image_classifier.fit(epochs=30, save_path=\"ImageClassifier/ImageClassifier5.h5\", show_training_results=True, \n",
    "                     dropout_rate=0.5, kernel_regularizer_l1=0.01, kernel_regularizer_l2=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_classifier = ImageClassifier(IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "image_classifier.fit(epochs=30, save_path=\"ImageClassifier/ImageClassifier6.h5\", show_training_results=True, \n",
    "                     dropout_rate=0.5, kernel_regularizer_l1=0.001, kernel_regularizer_l2=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_classifier = ImageClassifier(IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "image_classifier.fit(epochs=30, save_path=\"ImageClassifier/ImageClassifier7.h5\", show_training_results=True, \n",
    "                     dropout_rate=0.2, kernel_regularizer_l1=0.0, kernel_regularizer_l2=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_classifier = ImageClassifier(IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "image_classifier.fit(epochs=30, save_path=\"ImageClassifier/ImageClassifier8.h5\", show_training_results=True, \n",
    "                     dropout_rate=0.5, kernel_regularizer_l1=0.0, kernel_regularizer_l2=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_classifier = ImageClassifier(IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "image_classifier.fit(epochs=30, save_path=\"ImageClassifier/ImageClassifier9.h5\", show_training_results=True, \n",
    "                     dropout_rate=0.5, kernel_regularizer_l1=0.01, kernel_regularizer_l2=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_classifier = ImageClassifier(IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "image_classifier.fit(epochs=30, save_path=\"ImageClassifier/ImageClassifier10.h5\", show_training_results=True, \n",
    "                     dropout_rate=0.5, kernel_regularizer_l1=0.001, kernel_regularizer_l2=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_classifier = ImageClassifier(IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "image_classifier.fit(epochs=30, save_path=\"ImageClassifier/ImageClassifier11.h5\", show_training_results=True, \n",
    "                     dropout_rate=0.8, kernel_regularizer_l1=0.001, kernel_regularizer_l2=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_classifier = ImageClassifier(IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "image_classifier.fit(epochs=30, save_path=\"ImageClassifier/ImageClassifier12.h5\", show_training_results=True, \n",
    "                     dropout_rate=0.8, kernel_regularizer_l1=0.01, kernel_regularizer_l2=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_classifier = ImageClassifier(IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "image_classifier.fit(epochs=30, save_path=\"ImageClassifier/ImageClassifier13.h5\", show_training_results=True, \n",
    "                     dropout_rate=0.2, kernel_regularizer_l1=0.01, kernel_regularizer_l2=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_classifier = ImageClassifier(IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "image_classifier.fit(epochs=30, save_path=\"ImageClassifier/ImageClassifier14.h5\", show_training_results=True, \n",
    "                     dropout_rate=0.2, kernel_regularizer_l1=0.001, kernel_regularizer_l2=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#captionGenerator = CaptionGenerator(IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "#captionGenerator.generateCaption(\"D:/Cours/Cesi/A5/UE/Option - Data Science/Projet/Dataset soutenance/Text  (4).png\")\n",
    "#captionGenerator.generateCaption(\"D:/Cours/Cesi/A5/UE/Option - Data Science/Projet/Dataset soutenance/Paint  (1).jpg\", \n",
    "#                                 prediction_model_path=\"ImageClassifier/ImageClassifierFullPainting.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
